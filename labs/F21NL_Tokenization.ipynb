{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "## Lab session for the course \"Introduction to Natural Language Processing (F21NL)\""
      ],
      "metadata": {
        "id": "nEUgpaICX_FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization feels like a necessary evil for modern language models\n",
        "\n",
        "Tokens are the fundamental unit in modern language models, every corpus is measured in units of tokens, and everything nowadays is about how a model perceives these tokens.\n",
        "\n",
        "Tokenization refers to the process by which a raw string is converted into sequences of tokens and vice versa. This process is necessary in order to be able to plug text into modern language models but can be the root of many problems. With tokenization we are able to create indices for the embedding table to obtain vectors that feed into the language model. So these vectors are how the model perceives a token. **Which characters, words, or sub-words, should be combined together?** ğŸ¤”\n",
        "\n",
        "- In our previous labs we created a vocabulary of 64 possible characters that occur in English names, and a look-up embedding table containing one vector for each possible character. As a result, we implemented a small embedding table that represents all possible names. This was fine in our case where we tried to generate English names but for more complicated tasks, even small sentences will be represented by very long sequences making it hard for the model to predict the next token.\n",
        "\n",
        "- In the first coursework you have experimented with word-level embeddings, i.e vectors that correspond to a word after possible some pre-processing. If we follow this approach we may have a larger embedding table but our sequences will be shorter. However, at test time it is highly likely that a word is not present in our vocabulary. Even if we choose an <unk> token to represent this words we would have the same vector corresponding to multiple words that possibly have a completely different meaning\n",
        "\n",
        "**Can we have something in between?** ğŸ˜€\n",
        "\n",
        "If you have been paying attention to the second coursework, we used the sentencepiece library to operate on a chunk-level where chunks of words are constructed via the Word-Piece tokenizer first introduced in [BERT](https://aclanthology.org/N19-1423/?utm_campaign=The+Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_m9bbH_7ECE1h3lZ3D61TYg52rKpifVNjL4fvJ85uqggrXsWDBTB7YooFLJeNXHWqhvOyC) paper.\n",
        "\n",
        "\n",
        "In this lab we will implement the [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte-pair_encoding) (BPE) algorithim, a similar technique that compress texts using chunks and was introduced in the [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) paper.\n",
        "\n",
        "\n",
        "### The effect of tokenization\n",
        "\n",
        "If you have played with API-based language models like ChatGPT, Gemini, or Claude you have noticed that these models often may weird mistakes. In fact in many of these cases may look like problems of the model but can also be traced back to tokenization:\n",
        "\n",
        "* Performance on tasks with non-english languages is worse than tasks focusing exclusively on English\n",
        "* Spelling tasks\n",
        "* Coding tasks\n",
        "* Performance variation when using different structured input (JSON / YAML)\n",
        "\n",
        "\n",
        "Finally, note that even with BPE, tokenization is a learning algorithm meaning that we introduce a training step that essentially determines how the model perceives a sentence by chunking the sentence into multiple parts. As a result, training a model is never end-to-end!\n",
        "\n",
        "### Visualize tokenizers of modern language models:\n",
        "\n",
        "Open the [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app), use the gpt2 tokenizer from the top right menu, and put the example string below:\n",
        "\n",
        "```\n",
        "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
        "\n",
        "127 + 677 = 804\n",
        "1275 + 6773 = 8041\n",
        "\n",
        "Egg.\n",
        "I have an Egg.\n",
        "egg.\n",
        "EGG.\n",
        "\n",
        "# This is the korean translation of the first example sentence\n",
        "í† í°í™”ëŠ” LLMì˜ ì—¬ëŸ¬ íŠ¹ì§•ì˜ í•µì‹¬ì…ë‹ˆë‹¤. ë¬´ì‹œí•˜ì§€ ë§ˆì„¸ìš”.\n",
        "\n",
        "for i in range(1, 101):\n",
        "    if i % 3 == 0 and i % 5 == 0:\n",
        "        print(\"FizzBuzz\")\n",
        "    elif i % 3 == 0:\n",
        "        print(\"Fizz\")\n",
        "    elif i % 5 == 0:\n",
        "        print(\"Buzz\")\n",
        "    else:\n",
        "        print(i)\n",
        "```\n",
        "\n",
        "- This example string is tokenized into 300 tokens for example the word `Tokenization` is split into two tokens while `space-is` corresponds to a single token, so spaces can be part of a chunk!\n",
        "\n",
        "- The string 127 corresponds to a single token, while the number 677 actually corresponds to two individual tokens. So when asking a model to add these two numbers up, the number 677 will be represented using two tokens and so the model has to take this into account when predicting the answer.\n",
        "\n",
        "- The string `Egg` is represented using two tokens but in the follow-up sentence `I have an Egg` the `space-Egg` corresponds to a single token! Also notice that the lowercase `egg` corresponds to a different token id than `Egg`. So for the same concept, we have different token ids and the language model has to learn that these are actually corresponding to the same concept!\n",
        "\n",
        "- Non-English sentences are usually decomposed into longer sequences compared to english ones and that is because the chunks in the non-english sentence are more fine-grained, and so this bloats up the sequence length which leads to model problems for example vanishing gradients in recurrent networks or exceeding the context window of a Transformer as we already saw. In the example above, the first sentence is also translated into Korean. Notice that despite the fewer Korean characters, the outcome of the tokenization is a much larger sequence compared to the tokenization of the English sentence.\n",
        "\n",
        "- Similar, in the final example notice that the python identation results into very bloated sequences.\n",
        "\n",
        "Finally, swap to `cl100k_base` which is the GPT4 tokenizer and for comparison.\n"
      ],
      "metadata": {
        "id": "Q9r7zDyjTMqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to take strings and feed them into language models.\n",
        "\n",
        "For that we want to 1) `encode` these strings into sequences of integers (the token ids), 2) query the look-up embedding table to obtain the vectors, and then 3) provide these vectors to the language models.\n",
        "\n",
        "But we want to be able to do this for literally EVERY possible string, not just english characters or any other special characters like emojis"
      ],
      "metadata": {
        "id": "ylJ9XTZfh9QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xx = \"ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)\"\n",
        "print(xx)"
      ],
      "metadata": {
        "id": "Taz1t_DPbpW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, strings are sequences of [Unicode code points](https://en.wikipedia.org/wiki/Unicode) (???), aka a definition of ~ 160k characters as of right now covering 172 scripts."
      ],
      "metadata": {
        "id": "nLks9varipbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unicode code point for each character\n",
        "[ord(x)for x in xx]"
      ],
      "metadata": {
        "id": "Ym9u2PamXlhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already converted the string into a sequence of integers!\n",
        "\n",
        "**Question:** Why can't we just use this as it is, without any tokenization?\n",
        "\n",
        "<details>\n",
        "<summary><strong>Answer</strong></summary>\n",
        "One reason is that the vocabulary is quite long but more importantly the unicode standard is constantly evolving just as our language! New words/scripts are being added so it is not a stable representation to just use to a language model.\n"
      ],
      "metadata": {
        "id": "yErRyI2QjciL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the input string into bytes, but its not really readable\n",
        "print(xx.encode(\"utf-8\"))\n",
        "# More readable format, every element of the list represents a byte, i.e a value between 0 to 255\n",
        "print(list(xx.encode(\"utf-8\")))"
      ],
      "metadata": {
        "id": "VYBHVlIoUgtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Why can't we just use this as it is, without any tokenization?\n",
        "\n",
        "<details>\n",
        "<summary><strong>Answer</strong></summary>\n",
        "If we use a byte-level vocabulary we will end up with a small embedding table, but extremely long sequences! In the case of a Transformer model this is inefficient as it will not allow us to attend to previous text when making the prediction of the next token.\n"
      ],
      "metadata": {
        "id": "kAQ5dBBplUiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we want to be able to support a large vocabulary to be as efficient as possible in our langauge model that we can tune as a hyperparameter but we also want to stick to a byte-level format to be able to support to accomodate for all different languages!"
      ],
      "metadata": {
        "id": "JAknD0XkmjvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte-pair encoding algorithm"
      ],
      "metadata": {
        "id": "0KFG5jJKmtlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The following example is from [wikipedia](https://en.wikipedia.org/wiki/Byte-pair_encoding). Suppose that the string that we want to encode is:\n",
        "\n",
        "`aaabdaaabac`\n",
        "\n",
        "The idea of the byte-pair encoding is that we are going to find the pair of tokens that occur the most frequently and replace that pair with a single token that will be included in our vocabulary.\n",
        "\n",
        "So in this example the byte-pair `aa` occurs most often and so will be replaced by a new token, lets say `Z`. Now we replace every single occurence of `aa` by `Z`:\n",
        "\n",
        "```\n",
        "ZabdZabac\n",
        "Z=aa\n",
        "```\n",
        "\n",
        "So we had a sequence of 10 and a vocabulary of 4 characters, and we replaced it with a sequence of 9 tokens with a vocabulary of 4+1=5.\n",
        "\n",
        "Then we repeat the same process where the byte pair `ab` is replaced with `Y`:\n",
        "\n",
        "```\n",
        "ZYdZYac\n",
        "Y=ab\n",
        "Z=aa\n",
        "```\n",
        "\n",
        "The final byte-pair occurces only once and the algorithm stops here.\n",
        "\n",
        "So we start with byte-sequences with an intial vocabulary of 256 tokens. We iteratively and find the byte pairs that occur the most, creating new vocabulary tokens for the most common byte pair, and then replace all the occurences of the particular pair in the sequence."
      ],
      "metadata": {
        "id": "1nDd8Wshmx24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "PxTCbBBFUXwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ],
      "metadata": {
        "id": "c_vxc23bplV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVZQX7f2S_YD"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "print(word_freqs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1: Implement a function to find the most common pair of ids"
      ],
      "metadata": {
        "id": "Gabl2nezp-BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def get_counts(token_ids: list[str]) -> dict[tuple[int, int], int]:\n",
        "    \"\"\"Find the counts for all pairs of tokens in a list\"\"\"\n",
        "    raise NotImplementedError(\"You need to implement get_counts\")"
      ],
      "metadata": {
        "id": "zz2Izqs2qD3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
        "tokens = text.encode(\"utf-8\") # raw byte\n",
        "token_ids = list(map(int, tokens))\n",
        "\n",
        "counts = get_counts(token_ids=token_ids)\n",
        "top_pair = max(counts, key=counts.get)\n",
        "if not top_pair == (101, 32):\n",
        "    raise AssertionError(f\"Expecting the top_pair to be (101, 32), found {top_pair}\")\n",
        "\n",
        "min_pair = min(counts, key=counts.get)\n",
        "if not min_pair == (239, 188):\n",
        "    raise AssertionError(f\"Expecting the min_pair to be (239, 188), found {min_pair}\")\n"
      ],
      "metadata": {
        "id": "0GVZJQgjqjSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2: Implement a function to that merges the most common pair in a sequence of tokens"
      ],
      "metadata": {
        "id": "CJA-TVg6rkmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_token_ids(token_ids: list[int], pair: tuple[int, int], new_token_id: int) -> list[int]:\n",
        "    \"\"\"Merges a pair of token ids in a sequence and a single new token\n",
        "\n",
        "    Args:\n",
        "        token_ids (list[int]): A list of integers corresponding to the token ids\n",
        "        pair (tuple[int, int]): A tuple of of ids corresponding to the most frequently\n",
        "            occuring byte encoding pair\n",
        "        new_token_id (int): The new token id that we replace all pairs in the `token_ids` list\n",
        "\n",
        "    Returns:\n",
        "        new_token_ids (list[int]): A list of integers where each pair has been repalced by a new token id\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"You need to implement merge token ids\")"
      ],
      "metadata": {
        "id": "a14IWjlzrzng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_token_ids = merge_token_ids(\n",
        "    token_ids=[102, 12, 6, 7, 128, 13, 6, 7, 6, 7, 43, 1024], pair=(6, 7), new_token_id=42\n",
        ")\n",
        "\n",
        "if not new_token_ids == [102, 12, 42, 128, 13, 42, 42, 43, 1024]:\n",
        "    raise AssertionError(\n",
        "        f\"Expeting the new_token_ids to be [102, 12, 42, 128, 13, 42, 42, 43, 1024], but found {new_token_ids}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "fgEx-fdjs0gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop of the tokenizer\n",
        "\n",
        "We want to iterate between finding the most commonly occuring pair, and merging the token ids in a sequence. Implement a training function, e.g, a while loop that uses the two functions sequentially.\n",
        "\n",
        "How many times do we loop? This is a hyperparameter for us to pick. The more steps we take, the larger will be our vocabulary and the shorter will be our sequence."
      ],
      "metadata": {
        "id": "CshV6GaFt8jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making the training text longer to have more representative token statistics\n",
        "text = \"\"\"A Programmerâ€™s Introduction to Unicode March 3, 2017 Â· Coding Â· 22 Comments  ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡º\\u200cğŸ‡³\\u200cğŸ‡®\\u200cğŸ‡¨\\u200cğŸ‡´\\u200cğŸ‡©\\u200cğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, Iâ€™ll give an introduction to it from a programmerâ€™s point of view.  Iâ€™m going to focus on the character set and whatâ€™s involved in working with strings and files of Unicode text. However, in this article Iâ€™m not going to talk about fonts, text layout/shaping/rendering, or localization in detailâ€”those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And Moreâ€¦ Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. Itâ€™s not just that Unicode contains a much larger number of characters, although thatâ€™s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere â€œcharacter setâ€ to be. Weâ€™ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, itâ€™s hard not to find oneself asking, â€œWhy do we need all this? Is this really necessary? Couldnâ€™t it be simplified?â€  However, Unicode aims to faithfully represent the entire worldâ€™s writing systems. The Unicode Consortiumâ€™s stated goal is â€œenabling people around the world to use computers in any languageâ€. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and thereâ€™s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, itâ€™s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesnâ€™t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one textâ€”which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, youâ€™ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but donâ€™t be discouragedâ€”think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Letâ€™s start with some general orientation. The basic elements of Unicodeâ€”its â€œcharactersâ€, although that term isnâ€™t quite rightâ€”are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix â€œU+â€, such as U+0041 â€œAâ€ latin capital letter a or U+03B8 â€œÎ¸â€ greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of themâ€”about 12% of the codespaceâ€”are actually assigned, to date. Thereâ€™s plenty of room for growth! Unicode also reserves an additional 137,468 code points as â€œprivate useâ€ areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, itâ€™s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. Itâ€™s arranged in tiles for visual coherence; each small square is 16Ã—16 = 256 code points, and each large square is a â€œplaneâ€ of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the â€œBasic Multilingual Planeâ€, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no moreâ€”Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15â€“16 are reserved entirely for private use.  Scripts Letâ€™s zoom in on the first three planes, since thatâ€™s where the action is:  Map of scripts in Unicode planes 0â€“2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibilityâ€”itâ€™s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usageâ€”in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0â€“2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0â€“2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1â€“2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings Weâ€™ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = â€œUnicode Transformation Formatâ€), but itâ€™s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, youâ€™ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether itâ€™s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000â€“U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080â€“U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800â€“U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000â€“U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128â€“255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idiomsâ€”such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)â€”will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, itâ€™s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isnâ€™t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the â€œcharactersâ€ in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clustersâ€”more about those later), not bytes. When you measure the â€œlengthâ€ of a string, youâ€™ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that youâ€™re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000â€“U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000â€“U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called â€œsurrogatesâ€. All the code points in the range U+D800â€“U+DFFFâ€”or in other words, the code points that match the binary prefixes 110110 and 110111 in the table aboveâ€”are reserved specifically for UTF-16 encoding, and donâ€™t represent any valid characters on their own. Theyâ€™re only meant to occur in the 2-word encoding pattern above, which is called a â€œsurrogate pairâ€. Surrogate code points are illegal in any other context! Theyâ€™re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different â€œencodingsâ€; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didnâ€™t originally plan for. Surrogates were then introduced, asâ€”to put it bluntlyâ€”a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesnâ€™t support UTF-8â€”only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! ğŸ˜Š)  By the way, UTF-16â€™s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesnâ€™t match the systemâ€™s endianness, the BOM will be decoded as U+FFFE, which isnâ€™t a valid code point.)  Combining Marks In the story so far, weâ€™ve been focusing on code points. But in Unicode, a â€œcharacterâ€ can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabetâ€”and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called â€œcombining marksâ€, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character â€œÃâ€ can be expressed as a string of two code points: U+0041 â€œAâ€ latin capital letter a plus U+0301 â€œâ—ŒÌâ€ combining acute accent. This string automatically gets rendered as a single character: â€œÃâ€.  Now, Unicode does also include many â€œprecomposedâ€ code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 â€œÃâ€ latin capital letter a with acute or U+1EC7 â€œá»‡â€ latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they donâ€™t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by Í–ÍŸÍ…rÍaá¹‹Ì«Ì Ì–ÍˆÌ—dÍ–Ì»Ì¹Ã³mÌªÍ™Í•Ì—ÌÄ¼Í‡Ì°Í“Ì³Ì«Ã½Í“Ì¥ÌŸÍ Ì•sÌ«tÌ«Ì±Í•Ì—Ì°Ì¼Ì˜ÍœaÌ¼Ì©Í–Í‡Ì ÍˆÌ£ÍcÌ™ÍkÌ–Ì±Ì¹ÍÍ˜iÌ¢nÌ¨ÌºÌÍ‡Í‡ÌŸÍ™Ä£Ì«Ì®ÍÌ»ÌŸÍ… Ì•nÌ¼ÌºÍˆÍuÌ®Í™mÌºÌ­ÌŸÌ—ÍeÌÍ“Ì°Ì¤Í“Ì«rÌµoÌ–á¹·sÒ‰ÌªÍÌ­Ì¬ÌÌ¤ Ì®Í‰ÌÌÌ—ÌŸÍ dÌ´ÌŸÌœÌ±Í•ÍšiÍ‡Ì«Ì¼Ì¯Ì­ÌœÍ¡á¸Í™Ì»Ì¼cÌ²Ì²Ì¹rÌ¨Ì Ì¹Ì£Ì°Ì¦iÌ±tÌ¤Ì»Ì¤ÍÍ™Ì˜Ì•iÌµÌœÌ­Ì¤Ì±ÍcÌµs Í˜oÌ±Ì²ÍˆÌ™Í–Í‡Ì²Í¢nÍ˜ ÌœÍˆeÌ¬Ì²Ì Ì©acÍ•ÌºÌ Í‰hÌ·Ìª ÌºÌ£Í–Ì±á¸»Ì«Ì¬ÌÌ¹á¸™Ì™ÌºÍ™Ì­Í“Ì²tÌÌÍ‡Ì²Í‰ÍtÌ·Í”ÌªÍ‰Ì²Ì»Ì Í™eÌ¦Ì»ÍˆÍ‰Í‡rÍ‡Ì­Ì­Ì¬Í–,Ì–Ì ÌœÍ™Í“Ì£Ì­sÌ˜Ì˜ÍˆoÌ±Ì°Ì¤Ì²Í… Ì›Ì¬ÌœÌ™tÌ¼Ì¦Í•Ì±Ì¹Í•Ì¥hÌ³Ì²ÍˆÍÍ…aÌ¦tÌ»Ì² Ì»ÌŸÌ­Ì¦Ì–tÌ›Ì°Ì©hÌ Í•Ì³ÌÌ«Í•eÍˆÌ¤Ì˜Í–ÌÍ˜yÒ‰ÌÍ™ Ì·Í‰Í”Ì°Ì oÌÌ°vÍˆÍˆÌ³Ì˜ÍœerÌ¶fÌ°ÍˆÍ”á¸»Í•Ì˜Ì«ÌºÌ²oÌ²Ì­Í™Í Í…wÌ±Ì³Ìº ÍœtÌ¸hÍ‡Ì­Í•Ì³ÍeÌ–Ì¯ÌŸÌ  ÍÌÌœÍ”Ì©ÌªÍœÄ¼ÍÌªÌ²ÍšiÌÌ²Ì¹Ì™Ì©Ì¹nÌ¨Ì¦Ì©Ì–á¸™Ì¼Ì²Ì¼Í¢Í… Ì¬ÍsÌ¼ÍšÌ˜ÌÍpÍ™Ì˜Ì»aÌ™cÒ‰Í‰ÌœÌ¤ÍˆÌ¯Ì–iÌ¥Í¡nÌ¦Ì Ì±ÍŸgÌ¸Ì—Ì»Ì¦Ì­Ì®ÌŸÍ… Ì³ÌªÌ Í–Ì³Ì¯Ì•aÌ«ÍœnÍdÍ¡ Ì£Ì¦Ì™Í…cÌªÌ—rÌ´Í™Ì®Ì¦Ì¹Ì³eÍ‡ÍšÌÍ”Ì¹Ì«ÍŸaÌ™ÌºÌ™È›Í”ÍÌ˜Ì¹Í…eÌ¥Ì©Í aÍ–ÌªÌœÌ®Í™Ì¹nÌ¢Í‰Ì Í‡Í‰Í“Ì¦Ì¼ÌaÌ³Í–ÌªÌ¤Ì±pÌ–Í”Í”ÌŸÍ‡ÍÍ pÌ±ÍÌºÄ™Ì²ÍÍˆÌ°Ì²Ì¤Ì«aÌ¯ÍœrÌ¨Ì®Ì«Ì£Ì˜aÌ©Ì¯Í–nÌ¹Ì¦Ì°ÍÌ£ÌÌcÌ¨Ì¦Ì±Í”ÍÍÍ–eÌ¬Í“Í˜ Ì¤Ì°Ì©Í™Ì¤Ì¬Í™oÌµÌ¼Ì»Ì¬Ì»Í‡Ì®ÌªfÌ´ Ì¡Ì™Ì­Í“Í–ÌªÌ¤â€œÌ¸Í™Ì Ì¼cÌ³Ì—ÍœoÍÌ¼Í™Í”Ì®rÌÌ«ÌºÌÌ¥Ì¬ruÌºÌ»Ì¯Í‰Ì­Ì»Ì¯pÌ°Ì¥Í“Ì£Ì«Ì™Ì¤Í¢tÌ³ÍÌ³Ì–Í…iÌ¶ÍˆÌÍ™Ì¼Ì™Ì¹oÌ¡Í”nÌ™ÌºÌ¹Ì–Ì©ÍÍ…â€Ì¨Ì—Í–ÍšÌ©.Ì¯Í“  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, childrenâ€™s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\t×Ö¶×ª ×“Ö·×œÖ°×ªÖ´Ö¼×™ ×”Öµ×–Ö´×™×– ×”Öµ× Ö´×™×¢Ö·, ×§Ö¶×˜Ö¶×‘ ×œÖ´×©Ö°××›Ö·Ö¼×ªÖ´Ö¼×™ ×™Ö¸×©××•Ö¹×“ Normal writing (no niqqud):\\t××ª ×“×œ×ª×™ ×”×–×™×– ×”× ×™×¢, ×§×˜×‘ ×œ×©×›×ª×™ ×™×©×•×“ Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, â€œà¤¹â€ + â€œ\\u200bà¤¿â€ = â€œà¤¹à¤¿â€ (â€œhâ€ + â€œiâ€ = â€œhiâ€). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, itâ€™s also possible to dynamically compose them by concatenating their jamo. For example, â€œá„’â€ + â€œá…¡â€ + â€œá†«â€ = â€œí•œâ€ (â€œhâ€ + â€œaâ€ + â€œnâ€ = â€œhanâ€). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express â€œthe sameâ€ stringâ€”different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character â€œÃâ€ either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: â€œÇ¡â€ (dot, then macron) is different from â€œÄÌ‡â€ (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesnâ€™t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter â€œá»‡â€ can be expressed in five different ways:  Fully precomposed: U+1EC7 â€œá»‡â€ Partially precomposed: U+1EB9 â€œáº¹â€ + U+0302 â€œâ—ŒÌ‚â€ Partially precomposed: U+00EA â€œÃªâ€ + U+0323 â€œâ—ŒÌ£â€ Fully decomposed: U+0065 â€œeâ€ + U+0323 â€œâ—ŒÌ£â€ + U+0302 â€œâ—ŒÌ‚â€ Fully decomposed: U+0065 â€œeâ€ + U+0302 â€œâ—ŒÌ‚â€ + U+0323 â€œâ—ŒÌ£â€ Unicode refers to set of strings like this as â€œcanonically equivalentâ€. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a â€œfind in fileâ€ operation and the user searches for â€œá»‡â€, it should, by default, find occurrences of any of the five versions of â€œá»‡â€ above!  Normalization Forms To address the problem of â€œhow to handle canonically equivalent stringsâ€, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The â€œNFDâ€ normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesnâ€™t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The â€œNFCâ€ form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The â€œKâ€ here refers to compatibility decompositions, which cover characters that are â€œsimilarâ€ in some sense but not visually identical. However, Iâ€™m not going to cover that here.  Grapheme Clusters As weâ€™ve seen, Unicode contains various cases where a thing that a user thinks of as a single â€œcharacterâ€ might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single â€œuser-perceived characterâ€.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. Itâ€™s approximately â€œa base code point followed by any number of combining marksâ€, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: theyâ€™re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you canâ€™t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limitâ€”say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldnâ€™t want to enforce that by just truncating bytes. At a minimum, youâ€™d want to â€œround downâ€ to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And Moreâ€¦ Thereâ€™s much more that could be said about Unicode from a programmerâ€™s perspective! I havenâ€™t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issuesâ€”how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps Iâ€™ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and â€œcharactersâ€. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and itâ€™s clear that weâ€™re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)â€”C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fontsâ€”set of fonts intended to cover all assigned code points\"\"\"\n",
        "# raw bytes\n",
        "tokens = text.encode(\"utf-8\")\n",
        "# convert to a list of integers in range 0..255 for convenience\n",
        "tokens = list(map(int, tokens))\n",
        "print(f\"Length {(len(tokens))}\")"
      ],
      "metadata": {
        "id": "sNsFEbkTtzUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(token_ids: list[int], vocabulary_size: int):\n",
        "    if vocabulary_size <= 256:\n",
        "        raise ValueError(f\"Cannot compress the token ids to lower than 256 tokens\")\n",
        "\n",
        "    # copy so we don't destroy the original list\n",
        "    token_ids_copy = list(token_ids)\n",
        "    merges = {} # (int, int) -> int\n",
        "\n",
        "    num_merges = vocabulary_size - 256\n",
        "    for i in range(num_merges):\n",
        "        counts = get_counts(token_ids=token_ids_copy)\n",
        "        pair = max(counts, key=counts.get)\n",
        "        new_token_id = 256 + i\n",
        "        print(f\"merging {pair} into a new token {new_token_id}\")\n",
        "        token_ids_copy = merge_token_ids(token_ids_copy, pair, new_token_id)\n",
        "        merges[pair] = new_token_id\n",
        "\n",
        "    return token_ids_copy, merges\n",
        "\n",
        "(new_token_ids, merges) = training_loop(token_ids=tokens, vocabulary_size=276)"
      ],
      "metadata": {
        "id": "zQBQz65UvKAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(new_token_ids))\n",
        "print(f\"compression ratio: {len(tokens) / len(new_token_ids):.2f}X\")"
      ],
      "metadata": {
        "id": "q6Mp8cNdxXaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding / Decoding\n",
        "- Given a string what are the tokens\n",
        "- Given a sequence of integers between 0 and vocabulary_size what is the text"
      ],
      "metadata": {
        "id": "WcKliAEoyH6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, merges: dict[tuple[int, int], int]) -> list[str]:\n",
        "    \"\"\"Encode a text into token ids.\"\"\"\n",
        "    tokens = list(text.encode(\"utf-8\"))\n",
        "    while len(tokens) >= 2:\n",
        "        stats = get_counts(tokens)\n",
        "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "        # Noething else can be merged, return\n",
        "        if pair not in merges:\n",
        "            break\n",
        "\n",
        "        idx = merges[pair]\n",
        "        tokens = merge_token_ids(tokens, pair, idx)\n",
        "    return tokens\n",
        "\n",
        "def decode(token_ids: list[int], vocabulary: dict[int, bytes]) -> str:\n",
        "    \"\"\"Decode a list of token ids into a string.\"\"\"\n",
        "    tokens = b\"\".join(vocab[idx] for idx in token_ids)\n",
        "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "    return text\n",
        "\n",
        "\n",
        "print(encode(text=\"\", merges=merges))\n",
        "print(encode(text=\"\", merges=merges))\n",
        "\n",
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merges.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "print(decode(token_ids=[128], vocabulary=vocab))\n",
        "\n",
        "# The decode function should return the same string\n",
        "print(\n",
        "    decode(\n",
        "        encode(\"Please return the same string\", merges=merges), vocabulary=vocab\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "hQyihq5OyXk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}